{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AyMV71XZTeAN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b374932-1fc6-4929-82d5-8d6d36945831"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load TFWhisperForConditionalGeneration"
      ],
      "metadata": {
        "id": "ItgIfqs_UJk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperProcessor, WhisperFeatureExtractor, TFWhisperForConditionalGeneration\n",
        "import wave\n",
        "import numpy as np\n",
        "from scipy.signal import resample\n",
        "import tensorflow as tf\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
        "model = TFWhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
        "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"ko\", task=\"transcribe\")\n",
        "forced_decoder_ids # check token number"
      ],
      "metadata": {
        "id": "UypnQTHeT3qu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6762c1c-caaf-4e96-ce41-2b99a05417f1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "All PyTorch model weights were used when initializing TFWhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of TFWhisperForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFWhisperForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 50264), (2, 50359), (3, 50363)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load sample data"
      ],
      "metadata": {
        "id": "00ExtLhuUMmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# wav open\n",
        "with wave.open('output.wav', 'rb') as wav_file:\n",
        "    print(\"channel:\", wav_file.getnchannels())\n",
        "    print(\"sample sample rate:\", wav_file.getframerate())\n",
        "    print(\"frames:\", wav_file.getnframes())\n",
        "    sr = wav_file.getframerate()\n",
        "    frames = wav_file.readframes(wav_file.getnframes())\n",
        "    audio_data = np.frombuffer(frames, dtype=np.int16)\n",
        "\n",
        "# resample sample rate\n",
        "target_sample_rate = 16000\n",
        "if sr != 16000:\n",
        "    number_of_samples = round(len(audio_data) * float(target_sample_rate) / sr)\n",
        "    audio_data = resample(audio_data, number_of_samples).astype(np.float32)\n",
        "\n",
        "# whisper input\n",
        "inputs = processor(audio_data, sampling_rate=target_sample_rate,\n",
        "                   return_tensors=\"np\",\n",
        "                   do_normalize = True)\n",
        "\n",
        "input_features = inputs.input_features\n",
        "print(input_features.shape)"
      ],
      "metadata": {
        "id": "cFOLsNy7UEQ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fa37c29-ece3-42eb-9f62-3eb2a83e69a8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "channel: 1\n",
            "sample sample rate: 16000\n",
            "frames: 79872\n",
            "(1, 80, 3000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test generate Korean Transcribe"
      ],
      "metadata": {
        "id": "JBkmfO4bUsVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_map = [50258, 50264, 50359, 50363] # korean, transcribe\n",
        "decoder_input_ids = np.array([token_map])\n",
        "\n",
        "# forward encoder\n",
        "encoder_outputs = model.model.encoder(input_features)\n",
        "# forward decoder\n",
        "dout = model.model.decoder(input_ids= decoder_input_ids, encoder_hidden_states=encoder_outputs[0])\n",
        "# matmul embedding\n",
        "lm_logits = tf.matmul(dout[0], model.get_output_embeddings().weights, transpose_b=True)\n",
        "np.concatenate((decoder_input_ids[:,:1], np.argmax(lm_logits, axis = -1)), axis = -1)"
      ],
      "metadata": {
        "id": "4lo0aesVUDYN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b35dca97-e82b-4e7f-8770-3102d436cce6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[50258, 50264, 50358, 50363,  9491]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GenerateModel save"
      ],
      "metadata": {
        "id": "QTYSV4qTVYiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GenerateModel(tf.Module):\n",
        "    def __init__(self, model):\n",
        "        super(GenerateModel, self).__init__()\n",
        "        self.model = model\n",
        "    # input_signature encoder, decoder\n",
        "    @tf.function(input_signature=[\n",
        "        tf.TensorSpec(shape=[1, 80, 3000], dtype=tf.float32, name=\"input_features\"),\n",
        "        tf.TensorSpec(shape=[1, None], dtype=tf.int32, name=\"decoder_input_ids\")\n",
        "    ])\n",
        "    # serving\n",
        "    def serving(self, input_features, decoder_input_ids):\n",
        "        encoder_outputs = self.model.model.encoder(input_features)\n",
        "        lookup = self.model.get_output_embeddings().weights[0]\n",
        "\n",
        "        def condition(decoder_input_ids):\n",
        "            return tf.not_equal(decoder_input_ids[0, -1], 50257)\n",
        "\n",
        "        def body(decoder_input_ids):\n",
        "            dout = self.model.model.decoder(input_ids=decoder_input_ids, encoder_hidden_states=encoder_outputs[0])\n",
        "            lm_logits = tf.matmul(dout[0], lookup, transpose_b=True)\n",
        "            predicted_ids = tf.argmax(lm_logits, axis=-1, output_type=tf.int32)\n",
        "            decoder_input_ids = tf.concat([decoder_input_ids, predicted_ids[:, -1:]], axis=-1)\n",
        "            return decoder_input_ids\n",
        "\n",
        "        decoder_input_ids = tf.while_loop(condition, body, [decoder_input_ids], shape_invariants=[tf.TensorShape([1, None])])\n",
        "        return {\"seq\": tf.identity(decoder_input_ids)}\n",
        "# save tf model\n",
        "saved_model_dir = 'tf_whisper'\n",
        "generate_model = GenerateModel(model=model)\n",
        "tf.saved_model.save(generate_model, saved_model_dir, signatures={\"serving_default\": generate_model.serving})\n"
      ],
      "metadata": {
        "id": "YfUzYMYaVVkQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert saved model to TFLite model\n"
      ],
      "metadata": {
        "id": "1CUoSF3cV9ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tflite_model_path = 'whisper_base_gen.tflite'\n",
        "\n",
        "# convert model\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "converter.target_spec.supported_ops = [\n",
        "    tf.lite.OpsSet.TFLITE_BUILTINS,\n",
        "    tf.lite.OpsSet.SELECT_TF_OPS\n",
        "]\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.experimental_new_converter = True\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "# save tflite model\n",
        "with open(tflite_model_path, 'wb') as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "id": "F-N0Jm-kV89S"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate from TFLite model"
      ],
      "metadata": {
        "id": "yaheO8PtWY9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# load interpreter\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "print(input_details[0]['shape'])\n",
        "print(input_details[1]['shape'])\n",
        "print(output_details[0]['shape'])\n",
        "\n",
        "# decoder token with language, task\n",
        "token_map = [50258, 50264, 50359, 50363]\n",
        "decoder_input_ids = np.array([token_map])\n",
        "\n",
        "# generate\n",
        "decoder_input_ids = np.array([token_map]).astype(np.int32)\n",
        "interpreter.resize_tensor_input(input_details[0]['index'], decoder_input_ids.shape)\n",
        "interpreter.allocate_tensors()\n",
        "interpreter.set_tensor(input_details[1]['index'], input_features)\n",
        "interpreter.set_tensor(input_details[0]['index'], decoder_input_ids)\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "output_data"
      ],
      "metadata": {
        "id": "Cf9XS66kWUtU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c73392db-7e5c-4b98-b13f-72ae8601dfb6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1]\n",
            "[   1   80 3000]\n",
            "[1 1 1]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[50258, 50264, 50359, 50363,  9491,  9605,   242,   226,  3103,\n",
              "          8941,   235,   116, 22339, 40547,  8941,   235, 16270, 25575,\n",
              "         31253, 10134,  3833, 50257]]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# decode token\n",
        "transcription = processor.decode(output_data[0,0], skip_special_tokens=True)\n",
        "transcription"
      ],
      "metadata": {
        "id": "j6Ezw0bcXqVE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ae4cbca8-affe-4725-b339-24005a5fd776"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' 위스프어 모델 라이트 모델로 변환 예시'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}